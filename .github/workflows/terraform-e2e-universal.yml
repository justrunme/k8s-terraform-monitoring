name: Terraform E2E Universal Test

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  e2e:
    runs-on: ubuntu-latest
    timeout-minutes: 40

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Kind & Kubectl
        run: |
          curl -LO "https://kind.sigs.k8s.io/dl/v0.23.0/kind-linux-amd64"
          sudo install -m 0755 kind-linux-amd64 /usr/local/bin/kind
          curl -LO "https://dl.k8s.io/release/v1.29.0/bin/linux/amd64/kubectl"
          sudo install -m 0755 kubectl /usr/local/bin/kubectl

      - name: Create Kind Cluster
        run: kind create cluster --wait 60s

      - name: Wait for Cluster DNS (CoreDNS)
        run: kubectl rollout status deployment/coredns -n kube-system --timeout=90s

      - name: Prepare hostPath for PV (Kind)
        run: |
          docker exec kind-control-plane mkdir -p /tmp/demo-data
          docker exec kind-control-plane chmod 777 /tmp/demo-data

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init
        run: terraform init -backend=false

      - name: Terraform Apply (Helm ONLY)
        run: terraform apply -auto-approve -target=helm_release.kube_prometheus_stack

      - name: Wait for ServiceMonitor CRD
        run: |
          for i in {1..30}; do
            kubectl get crd servicemonitors.monitoring.coreos.com && exit 0
            echo "Waiting for ServiceMonitor CRD..."
            sleep 5
          done
          echo "CRD ServiceMonitor not found after timeout!" >&2
          exit 1

      - name: Terraform Apply (ALL)
        run: terraform apply -auto-approve

      - name: Wait for Monitoring Namespace
        run: for i in {1..30}; do kubectl get ns monitoring && break || sleep 4; done

      - name: Wait for all pods in monitoring namespace
        run: |
          echo "--- Waiting for all pods in 'monitoring' namespace to be ready ---"
          kubectl wait --for=condition=Ready pod --all -n monitoring --timeout=5m || true
          kubectl get pods -n monitoring

      - name: PVC Bound Test (if used)
        run: |
          kubectl get pvc -n monitoring || echo "No PVCs in monitoring ns"

      - name: Check test-app Service
        run: kubectl get svc test-app -n monitoring || echo "test-app Service not found"

      - name: Port-Forward & cURL Test (test-app)
        run: |
          SVC_PORT=$(kubectl get svc test-app -n monitoring -o jsonpath='{.spec.ports[0].port}' 2>/dev/null || echo "")
          if [ -z "$SVC_PORT" ]; then
            echo "test-app Service not found or has no ports, skipping test."
            exit 0
          fi
          kubectl port-forward svc/test-app 8080:$SVC_PORT -n monitoring &
          sleep 5
          curl -sf http://localhost:8080/metrics | grep -i "node_exporter_build_info" || (echo "No node-exporter metrics" && exit 1)

      - name: List all services in monitoring
        run: kubectl get svc -n monitoring

      - name: List all pods and events in monitoring namespace
        run: |
          echo "--- All pods in monitoring namespace ---"
          kubectl get pods -n monitoring -o wide
          echo "--- All events in monitoring namespace ---"
          kubectl get events -n monitoring --sort-by=.lastTimestamp

      - name: Port-forward Grafana & check login page
        run: |
          GRAFANA_SVC=$(kubectl get svc -n monitoring -o name | grep grafana | head -n1)
          if [ -z "$GRAFANA_SVC" ]; then
            echo "Grafana service not found, skipping Grafana login test."
            exit 0
          fi
          GRAFANA_SVC_NAME=${GRAFANA_SVC#service/}
          GRAFANA_PORT=$(kubectl get svc $GRAFANA_SVC_NAME -n monitoring -o jsonpath='{.spec.ports[0].port}')
          GRAFANA_POD=$(kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -z "$GRAFANA_POD" ]; then
            echo "Grafana pod not found, skipping port-forward/login test."
            exit 0
          fi
          kubectl port-forward -n monitoring $GRAFANA_SVC_NAME 30090:$GRAFANA_PORT &
          sleep 10
          curl -sf http://localhost:30090/login | grep -i "grafana" || (echo "No Grafana login page" && exit 1)

      - name: Show test-app Service Endpoints
        run: kubectl get endpoints test-app -n monitoring -o yaml || echo "No endpoints for test-app"

      - name: List all pods in monitoring namespace (short)
        run: kubectl get pods -n monitoring --show-labels

      - name: Get Prometheus Operator Pod Logs
        run: |
          PROM_OPERATOR_POD=$(kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus-operator -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ ! -z "$PROM_OPERATOR_POD" ]; then
            kubectl logs $PROM_OPERATOR_POD -n monitoring
          else
            echo "Prometheus Operator pod not found"
          fi

      - name: Verify Prometheus Target for test-app
        run: |
          PROMETHEUS_POD=$(kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -z "$PROMETHEUS_POD" ]; then
            echo "Prometheus pod not found! (not fatal for the workflow)"
            exit 0
          fi
          kubectl port-forward -n monitoring $PROMETHEUS_POD 9090:9090 &
          PORT_FORWARD_PID=$!
          sleep 5
          for i in {1..20}; do
            API_RESPONSE=$(curl -s http://localhost:9090/api/v1/targets)
            TARGET_HEALTH=$(echo $API_RESPONSE | jq -r '.data.activeTargets[] | select(.labels.job == "test-app" or .discoveredLabels.app == "test-app") | .health')
            if [ "$TARGET_HEALTH" = "up" ]; then
              echo "SUCCESS: Prometheus target 'test-app' is up!"
              kill $PORT_FORWARD_PID
              exit 0
            fi
            sleep 10
          done
          echo "FAILURE: Prometheus target 'test-app' was not found or not healthy after 200s."
          kill $PORT_FORWARD_PID
          exit 1

      - name: Verify Grafana Prometheus Datasource
        run: |
          GRAFANA_POD=$(kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -z "$GRAFANA_POD" ]; then
            echo "Grafana pod not found! (not fatal for the workflow)"
            exit 0
          fi
          GRAFANA_PASSWORD="prom-operator"  # Измени если пароль другой
          kubectl port-forward -n monitoring $GRAFANA_POD 3000:3000 &
          PORT_FORWARD_PID=$!
          sleep 5
          for i in {1..10}; do
            API_RESPONSE=$(curl -s --user "admin:$GRAFANA_PASSWORD" http://localhost:3000/api/datasources)
            DATASOURCE_ID=$(echo $API_RESPONSE | jq -r '.[] | select(.name == "Prometheus") | .id')
            if [ ! -z "$DATASOURCE_ID" ]; then
              echo "SUCCESS: Grafana datasource 'Prometheus' is configured! (id: $DATASOURCE_ID)"
              kill $PORT_FORWARD_PID
              exit 0
            fi
            sleep 10
          done
          echo "FAILURE: Grafana datasource 'Prometheus' not found or not healthy."
          echo "--- Full API Response ---"
          echo $API_RESPONSE | jq .
          kill $PORT_FORWARD_PID
          exit 1

      - name: Terraform Destroy
        if: ${{ always() }}
        run: terraform destroy -auto-approve || true

      - name: Delete Kind Cluster
        if: ${{ always() }}
        run: kind delete cluster || true
